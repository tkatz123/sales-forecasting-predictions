---
title: "Model Evaluation"
author: "Tyler Katz"
date: "2025-07-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load required packages
source("Utils/load_packages.R")
load_required_packages(c("tidyverse", "xgboost", "Metrics", "ggplot2", "readr"))
```

##LOADING MODEL & DATA
```{r}

#Loading XGBoost model
model <- readRDS("../Models/xgb_global_model.rds")

#Loading data for testing
load("../Data/preprocessed_train_data.RData")
test <- df

```

##TEST DATA PREPERATION

```{r, Warning = FALSE}

#Filters training data to match model training data
train <- df %>% filter(Date < as.Date("2015-06-15"))

#Filter test set to stores that are open and after specific date
test <- test %>%
  filter(Date >= as.Date("2015-06-15"))

#Convert categorical features to numeric as in training
to_numeric_factors <- function(df){
  df$StateHoliday <- as.numeric(df$StateHoliday)
  df$StoreType <- as.numeric(as.factor(df$StoreType))
  df$Assortment <- as.numeric(as.factor(df$Assortment))
  return(df)
}

#Apply function to test and train data
test <- to_numeric_factors(test)
train <- to_numeric_factors(train)

```

##PREPARE DMATRIX

```{r}

#Selecting variables from data to input into model
features <- c(
  "Store", "DayOfWeek", "Promo", "SchoolHoliday", "StateHoliday",
  "Year", "Month", "Day", "Week", "IsWeekend",
  "CompetitionDistance", "CompetitionOpenSinceMonth", "CompetitionOpenSinceYear",
  "Promo2SinceWeek", "Promo2SinceYear",
  "StoreType", "Assortment",
  "Sales_lag_1", "Sales_lag_7", "Sales_lag_14",
  "Sales_roll_mean_7", "Sales_roll_mean_14", "Open", "IsClosedDay",
  "IsMonthStart", "IsMonthEnd", "PromoActive", "CompetitionActive"
)

#Prepare DMatrixs
test_matrix <- xgb.DMatrix(data = as.matrix(test[, features]), label = test$Sales)
train_matrix <- xgb.DMatrix(data = as.matrix(train[, features]), label = train$Sales)

```

##MODEL PREDICTION
```{r}

#Make predictions using test data
preds <- predict(model, test_matrix)

#Making predictions using training data
train_preds <- predict(model, train_matrix)

```

##MODEL EVALUATION & VISUALIZATIONS
```{r}

#Calculate RMSE and MAE scores based on predictions and training data
train_rmse <- Metrics::rmse(train$Sales, train_preds)
train_mae <- Metrics::mae(train$Sales, train_preds)

#Print RMSE and MAE scores
cat("Train RMSE:", round(train_rmse, 2), "\n")
cat("Train MAE: ", round(train_mae, 2), "\n")

#Calculate RMSE and MAE scores based on predictions and testing data
rmse <- Metrics::rmse(test$Sales, preds)
mae <- Metrics::mae(test$Sales, preds)

#Print RMSE and MAE scores
cat("Validation RMSE:", round(rmse, 2), "\n")
cat("Validation MAE: ", round(mae, 2), "\n")

```

*Validation RMSE: 663.47*

  -Reflects the root mean squared error on unseen data, indicating that, on average, the model’s predictions deviate from actual sales by approximately 663 units.

  -This score demonstrates a strong ability to generalize to future observations, especially given the complexity and variability of retail sales data.

*Validation MAE: 438.57*

  -Represents the average absolute error between predicted and actual values.

  -This relatively low MAE confirms that the model is making consistently accurate predictions with minimal deviation across most test instances.

*Training vs. Validation Comparison*

  -The training RMSE (653.46) and validation RMSE (663.47) are nearly identical, suggesting the model is not overfitting and has learned generalized patterns rather than noise in the training data.

  -The moderate increase from training MAE (365.90) to validation MAE (438.57) is expected due to natural variation in unseen data, but the gap remains within acceptable bounds.

*Overall Interpretation*

  -The validation metrics indicate that the model performs reliably across a diverse range of stores and time periods.

  -The minimal difference between training and validation scores supports the conclusion that the model is well-tuned and appropriately regularized.

  -These results confirm that the model is robust and suitable for making forward-looking sales forecasts across the Rossmann store network.

```{r}

#Calculating residuals and saving in a DataFrame
residuals_df <- tibble(
  Actual = test$Sales,
  Predicted = preds,
  Residuals = test$Sales - preds
)

```

```{r}

#Residuals scatter plot
ggplot(residuals_df, aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.4, color = "steelblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Predicted", x = "Predicted Sales", y = "Residuals") +
  theme_minimal()

```

*Purpose*: Evaluates model accuracy by plotting the difference between actual and predicted sales.

*Ideal Pattern*: Residuals should be randomly distributed around zero with no clear trend.

*What We See*:

  -Most residuals cluster tightly around zero, indicating that the model makes accurate predictions for the majority of observations.

  -Residual spread is fairly uniform across the prediction range, with no pronounced funnel shape or heteroscedasticity.

  -A few moderate outliers are present, particularly for higher predicted sales, but they do not form a discernible trend.

*Interpretation*:

  -The model demonstrates strong consistency in its error distribution, with little evidence of bias or variance instability.

  -The horizontal band around zero suggests good calibration and generalization.

  -The presence of limited outliers implies opportunities for refinement on edge cases, but overall, the model is highly stable and well-tuned.

```{r}

#Actual vs predicted scatter plot
ggplot(residuals_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.4, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Predicted Sales", x = "Actual Sales", y = "Predicted Sales") +
  theme_minimal()

```
*Purpose*: Measures how closely the model’s predictions align with actual sales values.

*Ideal Pattern*: Points should lie close to the red 45° diagonal line, indicating perfect prediction.

*What We See*:

  -A dense linear cluster of points aligned with the red diagonal suggests strong correlation between actual and predicted sales.

  -Minimal dispersion is observed across most of the range, indicating consistently accurate predictions.

  -Slight fan-out at higher sales values, showing that variance increases modestly for large sales, though most predictions remain close to actuals.

  -Few outliers appear, but they are limited in number and not extreme enough to indicate instability.

*Interpretation*:

  -The model demonstrates excellent predictive performance across the full sales distribution.

  -The tight clustering around the diagonal highlights strong model generalization and robustness.

  -Minor over- and underestimation at high sales volumes may suggest opportunities for targeted refinement, such as segment-specific tuning or log transformation.

  -Overall, this plot validates the model's ability to predict store-level sales with high precision.

```{r, warning=FALSE}

#Created a predicted column in the test DataFrame with the predicted values for graphing
test$Predicted <- preds

#Aggregates sales and predictions per day across all stores
daily_sales <- test %>%
  group_by(Date) %>%
  summarise(Actual = sum(Sales), Predicted = sum(Predicted))

ggplot(daily_sales, aes(x = Date)) +
  geom_line(aes(y = Actual, color = "Actual"), size = 1) +
  geom_line(aes(y = Predicted, color = "Predicted"), size = 1) +
  labs(title = "Total Sales: Actual vs Predicted (Aggregated by Day)", 
       y = "Sales", x = "Date") +
  scale_color_manual(values = c("Actual" = "red", "Predicted" = "blue")) +
  theme_minimal()

```
*Purpose*: Evaluates how well the model captures overall sales trends across all stores on a daily basis.

*Ideal Pattern*: Predicted (blue) and actual (red) lines should closely overlap across time.

*What We See*:

  -The model shows excellent alignment with actual sales, particularly in capturing weekly sales cycles and daily fluctuations.

  -Sharp Sunday dips are modeled accurately, reflecting strong understanding of store closure patterns.

  -Prediction curves are consistently close to the actuals, even during high-variability days like weekends and promotion periods.

  -Minor deviations exist but are small relative to the total sales scale, suggesting reliable aggregate-level performance.

*Interpretation*:

  -The model effectively captures temporal seasonality and operational rhythms across the full store network.

  -The strong day-level fit validates the use of lagged sales, rolling averages, and calendar-based features (e.g., `DayOfWeek`, `Promo`, `IsWeekend`).

  -Overall, this graph demonstrates that the model generalizes well and can be trusted to forecast macro-level sales behavior with high fidelity.

```{r}

#Error distribution histogram
ggplot(residuals_df, aes(x = Residuals)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white") +
  labs(title = "Distribution of Residuals", x = "Residuals", y = "Frequency") +
  theme_minimal()
```
*Purpose*: Evaluates the overall distribution of prediction errors (residuals = actual − predicted).

*Ideal Pattern*: A tight, symmetric bell-shaped distribution centered at zero.

*What We See*:

  -An extremely tight cluster of residuals around 0, with the highest concentration in a narrow central bin.

  -Minimal presence of large residuals, confirming that significant prediction errors are rare.

  -The distribution is highly peaked and appears nearly symmetric, with very light tails on either side.

*Interpretation*:

  -The model produces consistently accurate predictions, with the majority of errors falling within a small range.

  -The lack of skewness and absence of heavy tails reinforces the assumption of stable model behavior and error variance.

  -Overall, this residual distribution is a strong indicator of high predictive precision and robustness.



```{r}

# Feature importance
importance <- xgb.importance(model = model)
xgb.plot.importance(importance_matrix = importance, top_n = 20)

```
This plot displays the relative importance of each input feature used by the XGBoost model during training. Importance is calculated based on how frequently a feature is used in tree splits and the associated gain in model performance.

*Key Insights*:

  -`Open` is the most dominant feature, accounting for nearly 30% of total importance. This makes sense, as a store being open is a prerequisite for sales.

  -`Sales_roll_mean_7` and `Sales_lag_14` are highly influential, confirming that short-term historical sales patterns are strong predictors of future sales.

  -`IsClosedDay`, derived from the `Open` feature, also ranks high—indicating it adds interpretability and value by distinguishing closed-store days.

  -Other temporal features like `DayOfWeek`, `Sales_lag_1`, and `Promo` contribute meaningfully, supporting the significance of weekly cycles and promotional activity.

  -Lower-ranked features include `StoreType`, `Year`, and `SchoolHoliday`, suggesting they contribute minimally to the model's predictive power and could be revisited for potential pruning.

*Overall Interpretation*:

  -The model heavily relies on operational status and recent sales history, validating the importance of engineered time-series features.

  -The effectiveness of IsClosedDay highlights the benefit of transforming raw binary indicators into features that better reflect business logic.

  -This feature ranking supports continued refinement of input variables—streamlining the feature set could reduce model complexity without significantly harming performance.


##CONCLUDING THOUGHTS

This project set out to develop a robust global sales forecasting model for the Rossmann store network using historical time-series data. Leveraging XGBoost and extensive feature engineering, the model successfully captures key temporal, promotional, and operational sales dynamics across all 1,115 stores.

*Key Performance Metrics*

  -Final Model – Validation RMSE: 663.47

  -Final Model – Validation MAE: 438.57

  -(Baseline Model – RMSE: ~1450, MAE: ~1080)

These results represent a significant improvement over the baseline model. The inclusion of lag features, rolling averages, and a refined set of calendar-based indicators, combined with regularization and cross-validation, led to a well-generalized and accurate forecasting model.

*Feature Insights*

  -The most predictive features were `Open`, `Sales_roll_mean_7`, and `Sales_lag_14`, confirming the importance of recent sales trends and store operational status.

  -`IsClosedDay`, derived from `Open`, provided added clarity in distinguishing sales vs. no-sales days and emerged as a valuable feature.

  -Calendar variables such as `DayOfWeek`, `Promo`, and `IsWeekend` showed strong relevance in capturing sales seasonality and promotional effects.

  -Less influential features like `SchoolHoliday`, `StoreType`, and `Year` may be candidates for simplification in future iterations.

*Visual Diagnostics*

  -*Residual analysis* showed a tight, symmetric distribution centered around zero, with no major heteroscedasticity or systemic bias—indicating stable error behavior.

  -*Predicted vs. Actual scatter plots* revealed a strong linear relationship and minimal dispersion, validating the model's effectiveness across the full range of sales values.

  -*Daily aggregate plots* confirmed that the model accurately tracks weekly cycles, promotional events, and holiday-related dips, reinforcing the strength of the time-based feature set.

*Overall Assessment*

The final model demonstrates excellent generalization and captures complex retail sales behavior at both the individual store and aggregate levels. Diagnostic plots, validation scores, and feature importances all point to a well-calibrated and thoughtfully engineered solution. The integration of lag-based features, open/closed logic, and seasonality detection significantly enhanced forecast precision. This model provides a reliable foundation for forward-looking sales forecasting across the Rossmann network and can be further refined through targeted tuning or store-specific adaptations.



